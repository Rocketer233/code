by = "type"
[default]
r = 32
lora_alpha = 1
lora_dropout = 0.0
fan_in_fan_out = false # Set this to True if the layer to replace stores weight like (fan_in, fan_out)
is_target_conv_1d_layer = false
init_lora_weghts = false

[model_layer.self_attn.q_proj]
r = 32
lora_alpha = 1
lora_dropout = 0.0


[model_layer.self_attn.k_proj]
r = 32
lora_alpha = 1
lora_dropout = 0.0


[model_layer.self_attn.v_proj]
r = 32
lora_alpha = 1
lora_dropout = 0.0


[model_layer.self_attn.o_proj]
r = 32
lora_alpha = 1
lora_dropout = 0.0